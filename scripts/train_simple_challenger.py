#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæŒ‘æˆ˜è€…æ¨¡å‹è®­ç»ƒè„šæœ¬ - é¿å…å¤æ‚ä¾èµ–
"""

import argparse
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import mlflow
import mlflow.sklearn
import pickle
import os
from datetime import datetime
from pathlib import Path
import json

def setup_mlflow():
    """è®¾ç½®MLflow"""
    mlflow_uri = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5001')
    mlflow.set_tracking_uri(mlflow_uri)
    
    experiment_name = "simple_champion_challenger"
    try:
        # å°è¯•è·å–ç°æœ‰å®éªŒ
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment and experiment.lifecycle_stage == 'deleted':
            # å¦‚æœå®éªŒè¢«åˆ é™¤ï¼Œåˆ›å»ºæ–°çš„å®éªŒåç§°
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            experiment_name = f"simple_champion_challenger_{timestamp}"
            experiment_id = mlflow.create_experiment(experiment_name)
        elif experiment:
            experiment_id = experiment.experiment_id
        else:
            # å®éªŒä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
            experiment_id = mlflow.create_experiment(experiment_name)
    except Exception:
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å®éªŒ
        experiment_id = "0"
        experiment_name = "Default"

    mlflow.set_experiment(experiment_name)
    return experiment_id

def load_and_prepare_data(data_path):
    """åŠ è½½å’Œå‡†å¤‡æ•°æ®"""
    print(f"Loading data from: {data_path}")
    
    if not Path(data_path).exists():
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    df = pd.read_csv(data_path)
    print(f"Data shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")
    
    # æ£€æŸ¥ç›®æ ‡åˆ—
    if 'target' not in df.columns:
        print("Warning: 'target' column not found. Using last column as target.")
        target_col = df.columns[-1]
        df = df.rename(columns={target_col: 'target'})
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.drop('target', axis=1)
    y = df['target']
    
    # å¤„ç†åˆ†ç±»ç‰¹å¾
    categorical_columns = X.select_dtypes(include=['object']).columns
    for col in categorical_columns:
        X[col] = pd.Categorical(X[col]).codes
    
    # å¤„ç†ç¼ºå¤±å€¼
    X = X.fillna(X.mean())
    
    print(f"Features shape: {X.shape}")
    print(f"Target distribution: {y.value_counts().to_dict()}")
    
    return X, y

def train_model(X, y, model_type='random_forest'):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"Training {model_type} model...")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # é€‰æ‹©æ¨¡å‹
    if model_type == 'random_forest':
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
    elif model_type == 'logistic_regression':
        model = LogisticRegression(
            random_state=42,
            max_iter=1000
        )
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train)
    
    # é¢„æµ‹
    y_pred = model.predict(X_test)
    
    # è®¡ç®—æŒ‡æ ‡
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred, average='weighted'),
        'recall': recall_score(y_test, y_pred, average='weighted'),
        'f1_score': f1_score(y_test, y_pred, average='weighted')
    }
    
    print("Model performance:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")
    
    return model, metrics, X_test, y_test

def log_to_mlflow(model, metrics, model_type, model_name):
    """è®°å½•åˆ°MLflow"""
    print("Logging to MLflow...")
    
    with mlflow.start_run(run_name=f"{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # è®°å½•å‚æ•°
        mlflow.log_param("model_type", model_type)
        mlflow.log_param("model_name", model_name)
        mlflow.log_param("timestamp", datetime.now().isoformat())
        
        if hasattr(model, 'get_params'):
            for param, value in model.get_params().items():
                mlflow.log_param(f"model_{param}", value)
        
        # è®°å½•æŒ‡æ ‡
        for metric, value in metrics.items():
            mlflow.log_metric(metric, value)
        
        # ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°ï¼ˆé¿å…MLflowç‰ˆæœ¬å…¼å®¹é—®é¢˜ï¼‰
        import pickle
        import tempfile

        model_dir = Path("models") / model_name
        model_dir.mkdir(parents=True, exist_ok=True)

        model_path = model_dir / "model.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(model, f)

        # è®°å½•æ¨¡å‹è·¯å¾„ï¼ˆä¸ä¸Šä¼ artifactï¼‰
        mlflow.log_param("model_path", str(model_path))
        print(f"Model saved locally: {model_path}")
        
        # è®°å½•æ¨¡å‹ä¿¡æ¯åˆ°æ–‡ä»¶
        model_info = {
            'model_name': model_name,
            'model_type': model_type,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat(),
            'run_id': mlflow.active_run().info.run_id
        }
        
        # ä¿å­˜åˆ°æœ¬åœ°çŠ¶æ€æ–‡ä»¶
        state_dir = Path("state")
        state_dir.mkdir(exist_ok=True)
        
        with open(state_dir / f"{model_name}_info.json", 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print(f"Model logged with run ID: {mlflow.active_run().info.run_id}")
        return mlflow.active_run().info.run_id

def evaluate_against_champion(metrics, model_name):
    """ç®€å•çš„å† å†›æŒ‘æˆ˜è€…è¯„ä¼°"""
    print("\nEvaluating against champion...")
    
    state_dir = Path("state")
    champion_file = state_dir / "champion_model.json"
    
    if not champion_file.exists():
        print("No champion model found. This model becomes the champion!")
        
        champion_info = {
            'model_name': model_name,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat(),
            'status': 'champion'
        }
        
        with open(champion_file, 'w') as f:
            json.dump(champion_info, f, indent=2)
        
        return "promoted_to_champion"
    
    # åŠ è½½å½“å‰å† å†›
    with open(champion_file, 'r') as f:
        champion_info = json.load(f)
    
    champion_accuracy = champion_info['metrics']['accuracy']
    challenger_accuracy = metrics['accuracy']
    
    improvement = challenger_accuracy - champion_accuracy
    threshold = 0.01  # 1% improvement threshold
    
    print(f"Champion accuracy: {champion_accuracy:.4f}")
    print(f"Challenger accuracy: {challenger_accuracy:.4f}")
    print(f"Improvement: {improvement:.4f}")
    
    if improvement > threshold:
        print("ğŸ‰ Challenger performs better! Promoting to champion.")
        
        # å¤‡ä»½æ—§å† å†›
        backup_file = state_dir / f"champion_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(backup_file, 'w') as f:
            json.dump(champion_info, f, indent=2)
        
        # æ›´æ–°å† å†›
        new_champion_info = {
            'model_name': model_name,
            'metrics': metrics,
            'timestamp': datetime.now().isoformat(),
            'status': 'champion',
            'previous_champion': champion_info['model_name']
        }
        
        with open(champion_file, 'w') as f:
            json.dump(new_champion_info, f, indent=2)
        
        return "promoted_to_champion"
    else:
        print("Challenger does not meet improvement threshold. Champion remains unchanged.")
        return "challenger_rejected"

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Train a simple challenger model")
    parser.add_argument("--data-path", required=True, help="Path to training data CSV")
    parser.add_argument("--model-type", default="random_forest", 
                       choices=["random_forest", "logistic_regression"],
                       help="Type of model to train")
    parser.add_argument("--model-name", help="Name for the model")
    parser.add_argument("--auto-evaluate", action="store_true", 
                       help="Automatically evaluate against champion")
    
    args = parser.parse_args()
    
    # ç”Ÿæˆæ¨¡å‹åç§°
    if not args.model_name:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.model_name = f"{args.model_type}_challenger_{timestamp}"
    
    print("ğŸš€ Starting Simple Challenger Training")
    print("=" * 50)
    print(f"Data path: {args.data_path}")
    print(f"Model type: {args.model_type}")
    print(f"Model name: {args.model_name}")
    print(f"Auto evaluate: {args.auto_evaluate}")
    print("=" * 50)
    
    try:
        # è®¾ç½®MLflow
        experiment_id = setup_mlflow()
        print(f"MLflow experiment ID: {experiment_id}")
        
        # åŠ è½½æ•°æ®
        X, y = load_and_prepare_data(args.data_path)
        
        # è®­ç»ƒæ¨¡å‹
        model, metrics, X_test, y_test = train_model(X, y, args.model_type)
        
        # è®°å½•åˆ°MLflow
        run_id = log_to_mlflow(model, metrics, args.model_type, args.model_name)
        
        # è‡ªåŠ¨è¯„ä¼°
        if args.auto_evaluate:
            result = evaluate_against_champion(metrics, args.model_name)
            print(f"\nEvaluation result: {result}")
        
        print("\nğŸ‰ Training completed successfully!")
        print(f"Model: {args.model_name}")
        print(f"Run ID: {run_id}")
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        
        # æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
        mlflow_uri = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5001')
        print(f"\nğŸ“Š View results in MLflow: {mlflow_uri}")
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
