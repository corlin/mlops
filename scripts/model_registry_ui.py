#!/usr/bin/env python3
"""
æ¨¡å‹æ³¨å†Œè¡¨Web UI - ä½¿ç”¨Streamlitæ„å»º
"""

import streamlit as st
import pandas as pd
import mlflow
from mlflow.tracking import MlflowClient
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import json
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.lifecycle import ChampionChallengerManager


# é¡µé¢é…ç½®
st.set_page_config(
    page_title="MLOps Model Registry",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ–MLflow
@st.cache_resource
def init_mlflow():
    """åˆå§‹åŒ–MLflowå®¢æˆ·ç«¯"""
    mlflow_uri = os.getenv('MLFLOW_TRACKING_URI', 'http://localhost:5000')
    mlflow.set_tracking_uri(mlflow_uri)
    return MlflowClient()

@st.cache_resource
def init_cc_manager():
    """åˆå§‹åŒ–å† å†›æŒ‘æˆ˜è€…ç®¡ç†å™¨"""
    config_path = "config/config.yaml"
    if Path(config_path).exists():
        return ChampionChallengerManager(config_path)
    return None

def get_registered_models(client):
    """è·å–æ³¨å†Œçš„æ¨¡å‹åˆ—è¡¨"""
    try:
        models = client.search_registered_models()
        return models
    except Exception as e:
        st.error(f"Failed to fetch registered models: {e}")
        return []

def get_model_versions(client, model_name):
    """è·å–æ¨¡å‹ç‰ˆæœ¬"""
    try:
        versions = client.search_model_versions(f"name='{model_name}'")
        return sorted(versions, key=lambda x: int(x.version), reverse=True)
    except Exception as e:
        st.error(f"Failed to fetch model versions: {e}")
        return []

def get_experiment_runs(experiment_name):
    """è·å–å®éªŒè¿è¡Œè®°å½•"""
    try:
        experiment = mlflow.get_experiment_by_name(experiment_name)
        if experiment:
            runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])
            return runs
        return pd.DataFrame()
    except Exception as e:
        st.error(f"Failed to fetch experiment runs: {e}")
        return pd.DataFrame()

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ¤– MLOps Model Registry Dashboard")
    st.markdown("---")
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = init_mlflow()
    cc_manager = init_cc_manager()
    
    # ä¾§è¾¹æ 
    st.sidebar.title("Navigation")
    page = st.sidebar.selectbox(
        "Choose a page",
        ["Overview", "Model Registry", "Experiments", "Champion-Challenger", "Monitoring"]
    )
    
    if page == "Overview":
        show_overview(client, cc_manager)
    elif page == "Model Registry":
        show_model_registry(client)
    elif page == "Experiments":
        show_experiments()
    elif page == "Champion-Challenger":
        show_champion_challenger(cc_manager)
    elif page == "Monitoring":
        show_monitoring(cc_manager)

def show_overview(client, cc_manager):
    """æ˜¾ç¤ºæ¦‚è§ˆé¡µé¢"""
    st.header("ğŸ“Š System Overview")
    
    col1, col2, col3, col4 = st.columns(4)
    
    # ç»Ÿè®¡ä¿¡æ¯
    try:
        models = get_registered_models(client)
        total_models = len(models)
        
        production_models = 0
        staging_models = 0
        
        for model in models:
            versions = get_model_versions(client, model.name)
            for version in versions:
                if version.current_stage == "Production":
                    production_models += 1
                elif version.current_stage == "Staging":
                    staging_models += 1
        
        with col1:
            st.metric("Total Models", total_models)
        
        with col2:
            st.metric("Production Models", production_models)
        
        with col3:
            st.metric("Staging Models", staging_models)
        
        with col4:
            if cc_manager:
                status = cc_manager.get_status()
                active_tests = len(status.get('active_shadow_tests', []))
                st.metric("Active Shadow Tests", active_tests)
            else:
                st.metric("Active Shadow Tests", "N/A")
        
    except Exception as e:
        st.error(f"Failed to load overview data: {e}")
    
    # ç³»ç»ŸçŠ¶æ€
    if cc_manager:
        st.subheader("ğŸ¯ Champion-Challenger Status")
        status = cc_manager.get_status()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Current Champion:**")
            champion = status.get('champion_model')
            if champion:
                st.json(champion)
            else:
                st.info("No champion model deployed")
        
        with col2:
            st.write("**Active Challengers:**")
            challengers = status.get('challenger_models', [])
            if challengers:
                for challenger in challengers:
                    st.json(challenger)
            else:
                st.info("No active challengers")

def show_model_registry(client):
    """æ˜¾ç¤ºæ¨¡å‹æ³¨å†Œè¡¨é¡µé¢"""
    st.header("ğŸ“‹ Model Registry")
    
    models = get_registered_models(client)
    
    if not models:
        st.info("No registered models found")
        return
    
    # æ¨¡å‹é€‰æ‹©
    model_names = [model.name for model in models]
    selected_model = st.selectbox("Select a model", model_names)
    
    if selected_model:
        st.subheader(f"Model: {selected_model}")
        
        # è·å–æ¨¡å‹ç‰ˆæœ¬
        versions = get_model_versions(client, selected_model)
        
        if versions:
            # ç‰ˆæœ¬è¡¨æ ¼
            version_data = []
            for version in versions:
                version_data.append({
                    "Version": version.version,
                    "Stage": version.current_stage,
                    "Created": datetime.fromtimestamp(version.creation_timestamp / 1000).strftime("%Y-%m-%d %H:%M:%S"),
                    "Run ID": version.run_id[:8] + "...",
                    "Description": version.description or "No description"
                })
            
            df = pd.DataFrame(version_data)
            st.dataframe(df, use_container_width=True)
            
            # ç‰ˆæœ¬è¯¦æƒ…
            selected_version = st.selectbox("Select version for details", [v.version for v in versions])
            
            if selected_version:
                version_detail = next(v for v in versions if v.version == selected_version)
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Version Information:**")
                    st.write(f"Version: {version_detail.version}")
                    st.write(f"Stage: {version_detail.current_stage}")
                    st.write(f"Run ID: {version_detail.run_id}")
                    st.write(f"Created: {datetime.fromtimestamp(version_detail.creation_timestamp / 1000)}")
                
                with col2:
                    st.write("**Tags:**")
                    if version_detail.tags:
                        for key, value in version_detail.tags.items():
                            st.write(f"{key}: {value}")
                    else:
                        st.write("No tags")
                
                # æ¨¡å‹é˜¶æ®µè½¬æ¢
                st.subheader("Stage Transition")
                new_stage = st.selectbox(
                    "Transition to stage",
                    ["None", "Staging", "Production", "Archived"],
                    index=0
                )
                
                if st.button("Transition Stage") and new_stage != "None":
                    try:
                        client.transition_model_version_stage(
                            name=selected_model,
                            version=selected_version,
                            stage=new_stage
                        )
                        st.success(f"Model transitioned to {new_stage}")
                        st.experimental_rerun()
                    except Exception as e:
                        st.error(f"Failed to transition stage: {e}")

def show_experiments():
    """æ˜¾ç¤ºå®éªŒé¡µé¢"""
    st.header("ğŸ§ª Experiments")
    
    # è·å–å®éªŒåˆ—è¡¨
    try:
        client = init_mlflow()
        experiments = client.search_experiments()
        
        if not experiments:
            st.info("No experiments found")
            return
        
        # å®éªŒé€‰æ‹©
        exp_names = [exp.name for exp in experiments if exp.name != "Default"]
        if not exp_names:
            st.info("No named experiments found")
            return
        
        selected_exp = st.selectbox("Select experiment", exp_names)
        
        if selected_exp:
            # è·å–è¿è¡Œè®°å½•
            runs_df = get_experiment_runs(selected_exp)
            
            if not runs_df.empty:
                st.subheader(f"Runs in {selected_exp}")
                
                # æ˜¾ç¤ºè¿è¡Œç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Total Runs", len(runs_df))
                
                with col2:
                    finished_runs = len(runs_df[runs_df['status'] == 'FINISHED'])
                    st.metric("Finished Runs", finished_runs)
                
                with col3:
                    failed_runs = len(runs_df[runs_df['status'] == 'FAILED'])
                    st.metric("Failed Runs", failed_runs)
                
                # è¿è¡Œè¡¨æ ¼
                display_columns = ['run_id', 'status', 'start_time', 'end_time']
                metric_columns = [col for col in runs_df.columns if col.startswith('metrics.')]
                display_columns.extend(metric_columns[:5])  # æ˜¾ç¤ºå‰5ä¸ªæŒ‡æ ‡
                
                if all(col in runs_df.columns for col in display_columns):
                    st.dataframe(runs_df[display_columns], use_container_width=True)
                
                # æŒ‡æ ‡å¯è§†åŒ–
                if metric_columns:
                    st.subheader("Metrics Visualization")
                    
                    selected_metric = st.selectbox("Select metric to visualize", metric_columns)
                    
                    if selected_metric:
                        fig = px.line(
                            runs_df, 
                            x='start_time', 
                            y=selected_metric,
                            title=f"{selected_metric} over time"
                        )
                        st.plotly_chart(fig, use_container_width=True)
            
            else:
                st.info(f"No runs found in experiment {selected_exp}")
    
    except Exception as e:
        st.error(f"Failed to load experiments: {e}")

def show_champion_challenger(cc_manager):
    """æ˜¾ç¤ºå† å†›æŒ‘æˆ˜è€…é¡µé¢"""
    st.header("ğŸ† Champion-Challenger Management")
    
    if not cc_manager:
        st.error("Champion-Challenger Manager not available. Check configuration.")
        return
    
    # è·å–çŠ¶æ€
    status = cc_manager.get_status()
    
    # å½“å‰çŠ¶æ€
    st.subheader("Current Status")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**Champion Model:**")
        champion = status.get('champion_model')
        if champion:
            st.json(champion)
        else:
            st.info("No champion model")
    
    with col2:
        st.write("**System Health:**")
        health = status.get('system_health', {})
        if health.get('overall_status') == 'healthy':
            st.success("System is healthy")
        else:
            st.warning("System has issues")
        
        if health.get('alerts'):
            for alert in health['alerts']:
                st.warning(alert)
    
    # æŒ‘æˆ˜è€…æ¨¡å‹
    st.subheader("Challenger Models")
    challengers = status.get('challenger_models', [])
    
    if challengers:
        for challenger in challengers:
            with st.expander(f"Challenger: {challenger['name']}"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.json(challenger)
                
                with col2:
                    if st.button(f"Evaluate {challenger['name']}", key=f"eval_{challenger['name']}"):
                        try:
                            with st.spinner("Evaluating challenger..."):
                                result = cc_manager.evaluate_challenger(challenger['name'])
                            st.success(f"Evaluation completed: {result.get('action', 'unknown')}")
                            st.experimental_rerun()
                        except Exception as e:
                            st.error(f"Evaluation failed: {e}")
    else:
        st.info("No challenger models")
    
    # å½±å­æµ‹è¯•
    st.subheader("Shadow Tests")
    shadow_tests = status.get('active_shadow_tests', [])
    
    if shadow_tests:
        for test in shadow_tests:
            with st.expander(f"Shadow Test: {test['challenger_name']}"):
                st.json(test)
    else:
        st.info("No active shadow tests")
    
    # æ‰‹åŠ¨æ“ä½œ
    st.subheader("Manual Operations")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("Run Lifecycle Cycle"):
            try:
                with st.spinner("Running lifecycle cycle..."):
                    result = cc_manager.run_lifecycle_cycle()
                st.success("Lifecycle cycle completed")
                st.json(result)
                st.experimental_rerun()
            except Exception as e:
                st.error(f"Lifecycle cycle failed: {e}")
    
    with col2:
        if st.button("Monitor Shadow Tests"):
            try:
                with st.spinner("Monitoring shadow tests..."):
                    results = cc_manager.monitor_shadow_tests()
                st.success("Shadow test monitoring completed")
                if results:
                    for result in results:
                        st.json(result)
                else:
                    st.info("No shadow tests to monitor")
            except Exception as e:
                st.error(f"Shadow test monitoring failed: {e}")

def show_monitoring(cc_manager):
    """æ˜¾ç¤ºç›‘æ§é¡µé¢"""
    st.header("ğŸ“ˆ Model Monitoring")
    
    if not cc_manager:
        st.error("Monitoring not available. Check configuration.")
        return
    
    # ç³»ç»Ÿå¥åº·çŠ¶æ€
    st.subheader("System Health")
    
    try:
        health = cc_manager.model_monitor.get_system_health()
        
        if health['overall_status'] == 'healthy':
            st.success("âœ… System is healthy")
        else:
            st.warning("âš ï¸ System has issues")
        
        # ç»„ä»¶çŠ¶æ€
        st.write("**Component Status:**")
        for component, status in health.get('components', {}).items():
            if status == 'healthy':
                st.success(f"{component}: {status}")
            else:
                st.error(f"{component}: {status}")
        
        # å‘Šè­¦
        alerts = health.get('alerts', [])
        if alerts:
            st.write("**Active Alerts:**")
            for alert in alerts:
                st.warning(alert)
    
    except Exception as e:
        st.error(f"Failed to get system health: {e}")
    
    # éƒ¨ç½²çŠ¶æ€
    st.subheader("Deployment Status")
    
    try:
        deployment_status = cc_manager.model_deployer.get_deployment_status()
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Champion Deployment:**")
            champion_deploy = deployment_status.get('champion')
            if champion_deploy:
                st.json(champion_deploy)
            else:
                st.info("No champion deployed")
        
        with col2:
            st.write("**Shadow Deployments:**")
            shadow_deploys = deployment_status.get('shadows', [])
            if shadow_deploys:
                for shadow in shadow_deploys:
                    st.json(shadow)
            else:
                st.info("No shadow deployments")
    
    except Exception as e:
        st.error(f"Failed to get deployment status: {e}")


if __name__ == "__main__":
    main()
