#!/usr/bin/env python3
"""
Ludwig evaluate()æ–¹æ³•æµ‹è¯•å·¥å…·
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def test_evaluate_return_values():
    """æµ‹è¯•Ludwig evaluate()æ–¹æ³•çš„è¿”å›å€¼"""
    print("ğŸ” æµ‹è¯•Ludwig evaluate()æ–¹æ³•è¿”å›å€¼...")
    
    try:
        from ludwig.api import LudwigModel
        print("âœ… Ludwigå¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥evaluateæ–¹æ³•çš„ç­¾å
        import inspect
        sig = inspect.signature(LudwigModel.evaluate)
        print(f"evaluateæ–¹æ³•ç­¾å: {sig}")
        
        # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
        if LudwigModel.evaluate.__doc__:
            print("evaluateæ–¹æ³•æ–‡æ¡£:")
            print(LudwigModel.evaluate.__doc__[:500] + "...")
        
    except ImportError:
        print("âŒ Ludwigæœªå®‰è£…")
        return False
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True

def create_safe_evaluate_wrapper():
    """åˆ›å»ºå®‰å…¨çš„evaluateåŒ…è£…å™¨"""
    print("\nğŸ’¡ å®‰å…¨çš„evaluateåŒ…è£…å™¨ä»£ç :")
    print("=" * 50)
    
    code = '''
def safe_evaluate_model(model, dataset_path, collect_predictions=True, collect_overall_stats=True):
    """å®‰å…¨çš„æ¨¡å‹è¯„ä¼°åŒ…è£…å™¨"""
    try:
        # è°ƒç”¨evaluateæ–¹æ³•
        eval_results = model.evaluate(
            dataset=dataset_path,
            collect_predictions=collect_predictions,
            collect_overall_stats=collect_overall_stats
        )
        
        # å¤„ç†ä¸åŒçš„è¿”å›å€¼æ ¼å¼
        test_results = None
        predictions = None
        additional_data = None
        
        if isinstance(eval_results, tuple):
            num_results = len(eval_results)
            print(f"Evaluate returned {num_results} values")
            
            if num_results >= 1:
                test_results = eval_results[0]
            if num_results >= 2:
                predictions = eval_results[1]
            if num_results >= 3:
                additional_data = eval_results[2]
                
        elif isinstance(eval_results, dict):
            # å¦‚æœè¿”å›å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
            test_results = eval_results
            
        else:
            # å…¶ä»–æƒ…å†µ
            test_results = eval_results
        
        return {
            'test_results': test_results,
            'predictions': predictions,
            'additional_data': additional_data,
            'success': True
        }
        
    except Exception as e:
        print(f"Evaluation failed: {e}")
        return {
            'test_results': None,
            'predictions': None,
            'additional_data': None,
            'success': False,
            'error': str(e)
        }
'''
    
    print(code)

def analyze_ludwig_versions():
    """åˆ†æLudwigç‰ˆæœ¬å·®å¼‚"""
    print("\nğŸ“Š Ludwigç‰ˆæœ¬å·®å¼‚åˆ†æ:")
    print("=" * 50)
    
    print("ä¸åŒLudwigç‰ˆæœ¬çš„evaluate()è¿”å›å€¼:")
    print("- Ludwig 0.6.x: (test_results, predictions)")
    print("- Ludwig 0.7.x: (test_results, predictions, output_directory)")
    print("- Ludwig 0.8.x: å¯èƒ½è¿”å›æ›´å¤šå€¼æˆ–ä¸åŒç»“æ„")
    print("- Ludwig 0.9.x+: è¿”å›å€¼ç»“æ„å¯èƒ½è¿›ä¸€æ­¥å˜åŒ–")
    
    print("\nå»ºè®®çš„å¤„ç†ç­–ç•¥:")
    print("1. ä½¿ç”¨tupleè§£åŒ…æ—¶æ£€æŸ¥é•¿åº¦")
    print("2. ä½¿ç”¨try-exceptå¤„ç†è§£åŒ…é”™è¯¯")
    print("3. æä¾›åå¤‡æ–¹æ¡ˆ")
    print("4. è®°å½•å®é™…è¿”å›å€¼ç»“æ„ä»¥ä¾¿è°ƒè¯•")

def generate_debug_code():
    """ç”Ÿæˆè°ƒè¯•ä»£ç """
    print("\nğŸ”§ è°ƒè¯•ä»£ç :")
    print("=" * 50)
    
    debug_code = '''
# åœ¨Ludwigè®­ç»ƒå™¨ä¸­æ·»åŠ è°ƒè¯•ä»£ç 
def debug_evaluate_return(model, dataset_path):
    """è°ƒè¯•evaluateè¿”å›å€¼"""
    try:
        eval_results = model.evaluate(
            dataset=dataset_path,
            collect_predictions=True,
            collect_overall_stats=True
        )
        
        print(f"Evaluateè¿”å›ç±»å‹: {type(eval_results)}")
        
        if isinstance(eval_results, tuple):
            print(f"Tupleé•¿åº¦: {len(eval_results)}")
            for i, item in enumerate(eval_results):
                print(f"  [{i}]: {type(item)} - {str(item)[:100]}...")
        elif isinstance(eval_results, dict):
            print(f"Dicté”®: {list(eval_results.keys())}")
        else:
            print(f"å…¶ä»–ç±»å‹: {eval_results}")
            
        return eval_results
        
    except Exception as e:
        print(f"è°ƒè¯•å¤±è´¥: {e}")
        return None
'''
    
    print(debug_code)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ Ludwig evaluate()æ–¹æ³•æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # æµ‹è¯•Ludwigå¯¼å…¥å’Œæ–¹æ³•ç­¾å
    ludwig_available = test_evaluate_return_values()
    
    # åˆ†æç‰ˆæœ¬å·®å¼‚
    analyze_ludwig_versions()
    
    # åˆ›å»ºå®‰å…¨åŒ…è£…å™¨
    create_safe_evaluate_wrapper()
    
    # ç”Ÿæˆè°ƒè¯•ä»£ç 
    generate_debug_code()
    
    print("\nğŸ¯ æ€»ç»“:")
    print("- evaluate()æ–¹æ³•è¿”å›å€¼åœ¨ä¸åŒLudwigç‰ˆæœ¬ä¸­å¯èƒ½ä¸åŒ")
    print("- éœ€è¦ä½¿ç”¨çµæ´»çš„è§£åŒ…ç­–ç•¥")
    print("- å»ºè®®ä½¿ç”¨try-exceptå¤„ç†è§£åŒ…é”™è¯¯")
    print("- å¯ä»¥æ·»åŠ è°ƒè¯•ä»£ç æ¥æ£€æŸ¥å®é™…è¿”å›å€¼ç»“æ„")
    
    if not ludwig_available:
        print("\nğŸ’¡ å½“å‰Ludwigä¸å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨:")
        print("python scripts/train_challenger_no_docker.py --data-path data/raw/baseline_full.csv --auto-evaluate")
    
    return ludwig_available

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
